{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC, SVC, SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import mixture\n",
    "from gtda.time_series import TakensEmbedding\n",
    "from PyEMD import EMD\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pylab import mpl\n",
    "from sklearn import preprocessing\n",
    "from scipy.io import arff\n",
    "import scipy\n",
    "import sklearn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from gtda.time_series import TakensEmbedding\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.31414157  0.98293484 -0.75803213 ... -1.1526735   0.18877102\n",
      "   0.87426702]\n",
      " [ 1.28966392 -0.62876061 -1.80715955 ...  0.41455061  1.16991954\n",
      "  -0.41075008]\n",
      " [-0.54314776 -1.84812     0.97218031 ...  1.44873086 -0.30172045\n",
      "   0.04984904]\n",
      " ...\n",
      " [-0.14901389 -0.16925385 -0.31443043 ...  0.07184478  0.63446649\n",
      "   0.38265176]\n",
      " [-0.28183117 -0.41974078 -1.5856026  ...  0.48165807  0.24244821\n",
      "   0.94789071]\n",
      " [-0.45027235 -1.62127262 -1.68265706 ...  0.21483519  0.92338687\n",
      "  -0.02493894]] (3838, 10)\n",
      "[0. 0. 1. ... 1. 0. 0.] (3838,)\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../dataset/data_akbilgic.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "data = data.iloc[:, 1:]\n",
    "data = data.to_numpy()\n",
    "data = data.transpose()\n",
    "\n",
    "X = np.zeros(10)\n",
    "Y = np.zeros(1)\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    line = data[i, :]\n",
    "    for j in range(line.shape[0]-10):\n",
    "        if np.any(line[j:j+11] == 0):\n",
    "            continue\n",
    "        else:\n",
    "            X = np.append(X, line[j:j+10])\n",
    "            Y = np.append(Y, line[j+10])\n",
    "\n",
    "X = X.reshape(-1, 10)\n",
    "Y = Y.reshape(-1, 1)\n",
    "\n",
    "X = np.delete(X, 0, axis=0)\n",
    "Y = np.delete(Y, 0, axis=0)\n",
    "\n",
    "f = np.frompyfunc(lambda x:1 if x>0 else 0, 1, 1)\n",
    "\n",
    "Y = f(Y)\n",
    "Y = Y.reshape(-1)\n",
    "\n",
    "X = sklearn.preprocessing.scale(X, axis=1)\n",
    "\n",
    "X = X.astype(float)\n",
    "Y = Y.astype(float)\n",
    "\n",
    "print(X, X.shape)\n",
    "print(Y, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.48611785 -1.34224483 -1.60439486 ... -0.87451141  0.82018893\n",
      "  -0.67706028]\n",
      " [ 0.2858818  -1.49271506 -1.07783688 ... -0.22550762  1.31583918\n",
      "   1.40890194]\n",
      " [-1.56019078  1.6810084  -0.11390562 ... -0.58669957  0.38397629\n",
      "   0.10598511]\n",
      " ...\n",
      " [ 1.45388763 -0.32291228 -2.35062325 ...  0.70221519 -0.20908321\n",
      "   0.70952613]\n",
      " [-0.21742435  1.65280399 -0.05275826 ... -0.84851823 -1.89938468\n",
      "   0.38584169]\n",
      " [ 2.46643456  1.16124943 -0.36838254 ...  0.14220902 -0.75339036\n",
      "  -0.62329881]]\n",
      "[1. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "print(X_train)\n",
    "print(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL part\n",
    "\n",
    "class VanillaRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layer=2, batch_first=True):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, \\\n",
    "            num_layers=num_layer, batch_first=batch_first)\n",
    "        self.mlp = torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size=1] need check\n",
    "        x = x.unsqueeze(2)    \n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.mlp(x)\n",
    "        x = x.view(-1, 1)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1\n",
    "seq_len = 10\n",
    "batch_size = 50\n",
    "hidden_size = 32\n",
    "epochs = 180\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train).to(device), torch.Tensor(Y_train).to(device)),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "model = VanillaRNN(input_size=input_size, hidden_size=hidden_size)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq).squeeze()\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Train Step:\", i, \" loss: \", single_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6099009900990099\n",
      "0.6312625250501002\n",
      "0.6\n",
      "0.615234375\n",
      "[[301 184]\n",
      " [210 315]]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = torch.utils.data.TensorDataset(torch.Tensor(X_test).to(device), torch.Tensor(Y_test).to(device)),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "pred = np.zeros(batch_size)\n",
    "true = np.zeros(batch_size)\n",
    "\n",
    "for seq, labels in test_loader:\n",
    "    y_pred = model(seq).squeeze()\n",
    "    pred = np.append(pred, np.round(y_pred.cpu().detach().numpy()))\n",
    "    true = np.append(true, labels.cpu().detach().numpy())\n",
    "\n",
    "print(accuracy_score(true, pred))\n",
    "print(precision_score(true, pred))\n",
    "print(recall_score(true, pred))\n",
    "print(f1_score(true, pred))\n",
    "print(sklearn.metrics.confusion_matrix(y_pred=pred, y_true=true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2878/2878 [00:01<00:00, 2408.39it/s]\n",
      "100%|██████████| 960/960 [00:00<00:00, 2774.49it/s]\n"
     ]
    }
   ],
   "source": [
    "te = TakensEmbedding(time_delay=1, dimension=5)\n",
    "sample_train_tda = te.fit_transform(X_train)\n",
    "sample_test_tda = te.fit_transform(X_test)\n",
    "\n",
    "feature_train_list = []\n",
    "feature_test_list = []\n",
    "\n",
    "for i in tqdm(range(sample_train_tda.shape[0])):\n",
    "    r = ripser(sample_train_tda[i, :, :])\n",
    "    feature = np.delete(r['dgms'][0], -1, axis=0)[:, 1]\n",
    "    feature2 = r['dgms'][1]\n",
    "    l = []\n",
    "    for i in range(feature2.shape[0]):\n",
    "        l.append(feature2[i][1]-feature2[i][0])\n",
    "    if feature2.shape[0]==0:\n",
    "        f_min = 0\n",
    "        f_max = 0\n",
    "        f_2ave = 0\n",
    "        f_2std = 0\n",
    "    else:\n",
    "        f_min = min(l)\n",
    "        f_max = max(l)\n",
    "        f_2ave = np.mean(feature2.flatten())\n",
    "        f_2std = np.std(feature2.flatten())\n",
    "    f = [feature.shape[0], np.sum(feature), np.mean(feature), np.std(feature), np.max(feature), np.min(feature), len([_ for _ in feature if _>0.5*np.max(feature)]), len([_ for _ in feature if _>np.mean(feature)]), f_min, f_max, f_2ave, f_2std]\n",
    "    f = [np.round(_, 2) for _ in f]\n",
    "    feature_train_list.append(np.array(f))\n",
    "\n",
    "for i in tqdm(range(sample_test_tda.shape[0])):\n",
    "    r = ripser(sample_test_tda[i, :, :])\n",
    "    feature = np.delete(r['dgms'][0], -1, axis=0)[:, 1]\n",
    "    f = [feature.shape[0], np.sum(feature), np.mean(feature), np.std(feature), np.max(feature), np.min(feature), len([_ for _ in feature if _>0.5*np.max(feature)]), len([_ for _ in feature if _>np.mean(feature)]), f_min, f_max, f_2ave, f_2std]\n",
    "    f = [np.round(_, 2) for _ in f]\n",
    "    feature_test_list.append(np.array(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# for i in range(20, 50):\n",
    "#     r = ripser(sample_train_tda[i, :, :])\n",
    "#     feature2 = r['dgms'][1]\n",
    "#     print(feature2)\n",
    "#     print(Y_train[i])\n",
    "\n",
    "i = 10\n",
    "r = ripser(sample_train_tda[i, :, :])\n",
    "feature2 = r['dgms'][1]\n",
    "print(feature2)\n",
    "print(Y_train[i])\n",
    "l = []\n",
    "for j in range(feature2.shape[0]):\n",
    "    l.append(feature2[j][1]-feature2[j][0])\n",
    "if feature2.shape[0]==0:\n",
    "    f_min = 0\n",
    "    f_max = 0\n",
    "    f_2ave = 0\n",
    "    f_2std = 0\n",
    "else:\n",
    "    f_min = min(l)\n",
    "    f_max = max(l)\n",
    "    f_2ave = np.mean(feature2.flatten())\n",
    "    f_2std = np.std(feature2.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.07811994]\n",
      " [0.         0.07915831]\n",
      " [0.         0.08569729]\n",
      " [0.         0.0934767 ]\n",
      " [0.         0.10649204]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.01674768]\n",
      " [0.         0.02128126]\n",
      " [0.         0.02933499]\n",
      " [0.         0.0295048 ]\n",
      " [0.         0.03006888]\n",
      " [0.                inf]]\n",
      "0.0\n",
      "[[0.         0.03405164]\n",
      " [0.         0.0353781 ]\n",
      " [0.         0.03558036]\n",
      " [0.         0.0360937 ]\n",
      " [0.         0.03831566]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.02493963]\n",
      " [0.         0.03632173]\n",
      " [0.         0.03670402]\n",
      " [0.         0.03862363]\n",
      " [0.         0.04017391]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.01344275]\n",
      " [0.         0.01570959]\n",
      " [0.         0.01663523]\n",
      " [0.         0.01890967]\n",
      " [0.         0.03240649]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.0107632 ]\n",
      " [0.         0.01768062]\n",
      " [0.         0.01897987]\n",
      " [0.         0.01930363]\n",
      " [0.         0.02363755]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.02328856]\n",
      " [0.         0.02670768]\n",
      " [0.         0.02882669]\n",
      " [0.         0.03116258]\n",
      " [0.         0.03375365]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.0219239 ]\n",
      " [0.         0.02272795]\n",
      " [0.         0.02354733]\n",
      " [0.         0.02390072]\n",
      " [0.         0.02952481]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.04394063]\n",
      " [0.         0.06392422]\n",
      " [0.         0.06708551]\n",
      " [0.         0.07023873]\n",
      " [0.         0.07321294]\n",
      " [0.                inf]]\n",
      "0.0\n",
      "[[0.         0.01684882]\n",
      " [0.         0.01741954]\n",
      " [0.         0.02046727]\n",
      " [0.         0.02274978]\n",
      " [0.         0.02474299]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.04584248]\n",
      " [0.         0.0534398 ]\n",
      " [0.         0.05878607]\n",
      " [0.         0.06727854]\n",
      " [0.         0.07253855]\n",
      " [0.                inf]]\n",
      "0.0\n",
      "[[0.         0.01479196]\n",
      " [0.         0.01688517]\n",
      " [0.         0.01875907]\n",
      " [0.         0.02080947]\n",
      " [0.         0.02321814]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.02306142]\n",
      " [0.         0.02362161]\n",
      " [0.         0.02692545]\n",
      " [0.         0.02762477]\n",
      " [0.         0.02781528]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.01214534]\n",
      " [0.         0.01405577]\n",
      " [0.         0.01664711]\n",
      " [0.         0.03130774]\n",
      " [0.         0.03779881]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.0194585 ]\n",
      " [0.         0.02141021]\n",
      " [0.         0.02202276]\n",
      " [0.         0.02207584]\n",
      " [0.         0.02370423]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.01177281]\n",
      " [0.         0.02540749]\n",
      " [0.         0.02848742]\n",
      " [0.         0.02893255]\n",
      " [0.         0.03811363]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.04283757]\n",
      " [0.         0.04347529]\n",
      " [0.         0.04596711]\n",
      " [0.         0.04681977]\n",
      " [0.         0.05433309]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.01475264]\n",
      " [0.         0.01994762]\n",
      " [0.         0.02609   ]\n",
      " [0.         0.03147364]\n",
      " [0.         0.03266787]\n",
      " [0.                inf]]\n",
      "0.0\n",
      "[[0.         0.01864676]\n",
      " [0.         0.02043732]\n",
      " [0.         0.02097342]\n",
      " [0.         0.02375942]\n",
      " [0.         0.02707759]\n",
      " [0.                inf]]\n",
      "1.0\n",
      "[[0.         0.00549234]\n",
      " [0.         0.01006983]\n",
      " [0.         0.01029963]\n",
      " [0.         0.01141463]\n",
      " [0.         0.01454021]\n",
      " [0.                inf]]\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20, 40):\n",
    "    print(ripser(sample_train_tda[i, :, :])['dgms'][0])\n",
    "    print(Y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_pd = pd.DataFrame(np.concatenate((np.array(feature_train_list), Y_train.reshape(-1, 1)), axis=1), \\\n",
    "    columns=['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average', 'fmin', 'fmax', 'f2ave', 'f2std', 'Class'])\n",
    "\n",
    "feature_test_pd = pd.DataFrame(np.concatenate((np.array(feature_test_list), Y_test.reshape(-1, 1)), axis=1), \\\n",
    "    columns=['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average', 'fmin', 'fmax', 'f2ave', 'f2std', 'Class'])\n",
    "\n",
    "# print(feature_train_pd.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_train_pd[['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average', 'fmin', 'fmax', 'f2ave', 'f2std']]\n",
    "# X_train = feature_train_pd[['Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', 'fmin', 'fmax']]\n",
    "Y_train = feature_train_pd[['Class']]\n",
    "\n",
    "X_test = feature_test_pd[['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average', 'fmin', 'fmax', 'f2ave', 'f2std']]\n",
    "# X_test = feature_test_pd[['Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', 'fmin', 'fmax']]\n",
    "Y_test = feature_test_pd[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5364583333333334\n",
      "0.5364583333333334\n",
      "1.0\n",
      "0.6983050847457627\n",
      "[[  0 445]\n",
      " [  0 515]]\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=0.1, kernel='poly')\n",
    "clf.fit(X_train, np.ravel(Y_train))\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(precision_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(recall_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(f1_score(y_pred=Y_pred, y_true=Y_test))\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(y_pred=Y_pred, y_true=Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5614583333333333\n",
      "0.5614583333333333\n",
      "1.0\n",
      "0.7191460973982654\n",
      "[[  0 421]\n",
      " [  0 539]]\n"
     ]
    }
   ],
   "source": [
    "clf = mixture.GaussianMixture(n_components=2)\n",
    "clf.fit(X_train)\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(precision_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(recall_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(f1_score(y_pred=Y_pred, y_true=Y_test))\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(y_pred=Y_pred, y_true=Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x21e9faec3c8>"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_np = X_train.to_numpy()\n",
    "Y_np = Y_train.to_numpy()\n",
    "zeros = np.where(Y_np == 0)[0]\n",
    "ones = np.where(Y_np == 1)[0]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "plt.scatter(pca.fit_transform(X_np[zeros])[:, 0], pca.fit_transform(X_np[zeros])[:, 1] ,color='red')\n",
    "plt.scatter(pca.fit_transform(X_np[ones])[:, 0], pca.fit_transform(X_np[ones])[:, 1], color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5479166666666667\n",
      "0.5479166666666667\n",
      "1.0\n",
      "0.7079407806191118\n",
      "[[  0 434]\n",
      " [  0 526]]\n"
     ]
    }
   ],
   "source": [
    "Y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(precision_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(recall_score(y_pred=Y_pred, y_true=Y_test))\n",
    "print(f1_score(y_pred=Y_pred, y_true=Y_test))\n",
    "\n",
    "print(sklearn.metrics.confusion_matrix(y_pred=Y_pred, y_true=Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaCLF(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=True):\n",
    "        super(VanillaCLF, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "        self.mlp1 = torch.nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        self.mlp2 = torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.ReLU = torch.nn.ReLU()\n",
    "        self.Sigm = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size=1] need check   \n",
    "        x = self.mlp1(x)\n",
    "        x = self.Sigm(x)\n",
    "        x = self.mlp2(x)\n",
    "        \n",
    "        return self.Sigm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.Tensor(X_train.to_numpy()).to(device))\n",
    "print(torch.Tensor(Y_train.to_numpy()).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 0  loss:  tensor(0.7032, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 1  loss:  tensor(0.6940, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 2  loss:  tensor(0.6893, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 3  loss:  tensor(0.6850, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 4  loss:  tensor(0.6467, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 5  loss:  tensor(0.6728, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 6  loss:  tensor(0.6634, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 7  loss:  tensor(0.6995, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 8  loss:  tensor(0.6923, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 9  loss:  tensor(0.6998, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 10  loss:  tensor(0.6666, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 11  loss:  tensor(0.6969, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 12  loss:  tensor(0.6673, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 13  loss:  tensor(0.6978, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 14  loss:  tensor(0.6842, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 15  loss:  tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 16  loss:  tensor(0.6626, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 17  loss:  tensor(0.6922, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 18  loss:  tensor(0.7590, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 19  loss:  tensor(0.6914, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 20  loss:  tensor(0.6956, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 21  loss:  tensor(0.6928, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 22  loss:  tensor(0.6804, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 23  loss:  tensor(0.6961, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 24  loss:  tensor(0.7044, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 25  loss:  tensor(0.7082, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 26  loss:  tensor(0.6859, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 27  loss:  tensor(0.6627, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 28  loss:  tensor(0.7267, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 29  loss:  tensor(0.6561, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 30  loss:  tensor(0.6989, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 31  loss:  tensor(0.7269, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 32  loss:  tensor(0.6886, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 33  loss:  tensor(0.6913, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 34  loss:  tensor(0.6918, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 35  loss:  tensor(0.7058, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 36  loss:  tensor(0.7005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 37  loss:  tensor(0.6718, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 38  loss:  tensor(0.6775, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 39  loss:  tensor(0.6873, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 40  loss:  tensor(0.7388, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 41  loss:  tensor(0.6596, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 42  loss:  tensor(0.6968, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 43  loss:  tensor(0.6904, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 44  loss:  tensor(0.6756, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 45  loss:  tensor(0.6908, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 46  loss:  tensor(0.7000, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 47  loss:  tensor(0.7131, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 48  loss:  tensor(0.6701, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 49  loss:  tensor(0.6938, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 50  loss:  tensor(0.6793, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 51  loss:  tensor(0.6696, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 52  loss:  tensor(0.6841, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 53  loss:  tensor(0.6925, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 54  loss:  tensor(0.6932, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 55  loss:  tensor(0.6994, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 56  loss:  tensor(0.6610, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 57  loss:  tensor(0.6787, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 58  loss:  tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 59  loss:  tensor(0.6676, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 60  loss:  tensor(0.7189, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 61  loss:  tensor(0.7202, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 62  loss:  tensor(0.7252, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 63  loss:  tensor(0.6953, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 64  loss:  tensor(0.6845, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 65  loss:  tensor(0.7424, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 66  loss:  tensor(0.7019, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 67  loss:  tensor(0.7139, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 68  loss:  tensor(0.6695, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 69  loss:  tensor(0.6762, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 70  loss:  tensor(0.6704, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 71  loss:  tensor(0.7290, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 72  loss:  tensor(0.6524, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 73  loss:  tensor(0.6906, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 74  loss:  tensor(0.6882, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 75  loss:  tensor(0.6748, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 76  loss:  tensor(0.6838, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 77  loss:  tensor(0.6758, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 78  loss:  tensor(0.6977, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 79  loss:  tensor(0.6947, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 80  loss:  tensor(0.7163, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 81  loss:  tensor(0.6685, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 82  loss:  tensor(0.7232, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 83  loss:  tensor(0.7050, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 84  loss:  tensor(0.6516, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 85  loss:  tensor(0.6916, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 86  loss:  tensor(0.6722, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 87  loss:  tensor(0.6814, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 88  loss:  tensor(0.6947, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 89  loss:  tensor(0.6948, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 90  loss:  tensor(0.6915, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 91  loss:  tensor(0.6944, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 92  loss:  tensor(0.6810, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 93  loss:  tensor(0.6528, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 94  loss:  tensor(0.6832, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 95  loss:  tensor(0.6389, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 96  loss:  tensor(0.7078, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 97  loss:  tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 98  loss:  tensor(0.6818, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 99  loss:  tensor(0.7447, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 100  loss:  tensor(0.7069, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 101  loss:  tensor(0.6934, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 102  loss:  tensor(0.6766, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 103  loss:  tensor(0.6747, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 104  loss:  tensor(0.7142, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 105  loss:  tensor(0.6909, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 106  loss:  tensor(0.6951, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 107  loss:  tensor(0.7140, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 108  loss:  tensor(0.6809, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 109  loss:  tensor(0.6644, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 110  loss:  tensor(0.6776, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 111  loss:  tensor(0.6680, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 112  loss:  tensor(0.7148, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 113  loss:  tensor(0.6909, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 114  loss:  tensor(0.6764, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 115  loss:  tensor(0.6749, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 116  loss:  tensor(0.6907, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 117  loss:  tensor(0.6682, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 118  loss:  tensor(0.6894, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 119  loss:  tensor(0.6528, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 120  loss:  tensor(0.6791, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 121  loss:  tensor(0.6991, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 122  loss:  tensor(0.7076, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 123  loss:  tensor(0.6888, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 124  loss:  tensor(0.7093, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 125  loss:  tensor(0.7032, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 126  loss:  tensor(0.6547, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 127  loss:  tensor(0.6743, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 128  loss:  tensor(0.7014, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 129  loss:  tensor(0.6514, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 130  loss:  tensor(0.6623, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 131  loss:  tensor(0.7621, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 132  loss:  tensor(0.6598, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 133  loss:  tensor(0.6741, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 134  loss:  tensor(0.7046, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 135  loss:  tensor(0.6561, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 136  loss:  tensor(0.7064, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 137  loss:  tensor(0.6851, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 138  loss:  tensor(0.7005, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 139  loss:  tensor(0.6577, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 140  loss:  tensor(0.6831, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 141  loss:  tensor(0.6672, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 142  loss:  tensor(0.7034, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 143  loss:  tensor(0.6972, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 144  loss:  tensor(0.6673, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 145  loss:  tensor(0.7158, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 146  loss:  tensor(0.6858, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 147  loss:  tensor(0.6934, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 148  loss:  tensor(0.6920, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 149  loss:  tensor(0.6969, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 150  loss:  tensor(0.7147, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 151  loss:  tensor(0.6672, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 152  loss:  tensor(0.6754, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 153  loss:  tensor(0.6941, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 154  loss:  tensor(0.7417, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 155  loss:  tensor(0.6679, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 156  loss:  tensor(0.6746, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 157  loss:  tensor(0.7048, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 158  loss:  tensor(0.7086, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 159  loss:  tensor(0.7078, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 160  loss:  tensor(0.6827, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 161  loss:  tensor(0.6946, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 162  loss:  tensor(0.6743, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 163  loss:  tensor(0.7067, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 164  loss:  tensor(0.6723, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 165  loss:  tensor(0.6634, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 166  loss:  tensor(0.6857, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 167  loss:  tensor(0.6703, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 168  loss:  tensor(0.6826, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 169  loss:  tensor(0.7239, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 170  loss:  tensor(0.6858, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 171  loss:  tensor(0.6617, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 172  loss:  tensor(0.6724, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 173  loss:  tensor(0.7041, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 174  loss:  tensor(0.6487, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 175  loss:  tensor(0.6735, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 176  loss:  tensor(0.6888, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 177  loss:  tensor(0.6639, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 178  loss:  tensor(0.6711, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 179  loss:  tensor(0.6078, grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_size = 12\n",
    "batch_size = 50\n",
    "hidden_size = 256\n",
    "epochs = 180\n",
    "# device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\"\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = torch.utils.data.TensorDataset(torch.Tensor(X_train.to_numpy()).to(device), torch.Tensor(Y_train.to_numpy()).to(device)),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "model = VanillaCLF(input_size=input_size, hidden_size=hidden_size)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq).squeeze()\n",
    "        # print(y_pred, labels)\n",
    "        single_loss = loss_function(y_pred, labels.squeeze())\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Train Step:\", i, \" loss: \", single_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64), array([], dtype=int64))\n",
      "(array([], dtype=int64), array([], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "yt = Y_train.to_numpy()\n",
    "print(np.where(yt>1))\n",
    "print(np.where(yt<0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ripser_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d856ea83737ecf31b084a6c4dd25de1f47360aa39d05be84e9035ad6a44a6c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
