{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC, SVC, SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from gtda.time_series import TakensEmbedding\n",
    "from PyEMD import EMD\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pylab import mpl\n",
    "from sklearn import preprocessing\n",
    "from scipy.io import arff\n",
    "import scipy\n",
    "import sklearn\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from gtda.time_series import TakensEmbedding\n",
    "from gtda.diagrams import BettiCurve\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14980, 15)\n"
     ]
    }
   ],
   "source": [
    "filepath = \"../dataset/EEGEyeState.arff\"\n",
    "data = arff.loadarff(filepath)\n",
    "df = pd.DataFrame(data[0])\n",
    "\n",
    "sample = df.values[:, 0:len(df.values[0])-1]\n",
    "label = df.values[:, -1].astype(int).reshape(-1,1)\n",
    "data = np.concatenate((sample, label),axis=1)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.06779371 -1.64898784 -0.14680398 ... -0.19358628  1.71306058\n",
      "   0.41447629]\n",
      " [ 0.06355243 -1.64953356 -0.10117149 ... -0.17804623  1.71346838\n",
      "   0.38197229]\n",
      " [ 0.08411988 -1.65479873 -0.09089869 ... -0.16310536  1.71475557\n",
      "   0.4174731 ]\n",
      " ...\n",
      " [-0.05419791 -1.54833733 -0.2145728  ... -0.15578084  1.58421115\n",
      "   0.26921002]\n",
      " [-0.0467027  -1.57767428 -0.22101063 ... -0.13788237  1.58339541\n",
      "   0.29914223]\n",
      " [-0.05442066 -1.57772692 -0.19974487 ... -0.12437991  1.57390319\n",
      "   0.27663931]]\n"
     ]
    }
   ],
   "source": [
    "sample = sklearn.preprocessing.scale(sample, axis=1)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DL part\n",
    "\n",
    "class VanillaRNN(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layer=2, batch_first=True):\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layer = num_layer\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size=input_size, hidden_size=hidden_size, \\\n",
    "            num_layers=num_layer, batch_first=batch_first)\n",
    "        self.mlp = torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_size=1] need check\n",
    "        x = x.unsqueeze(2)    \n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.mlp(x)\n",
    "        x = x.view(-1, 1)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Step: 0  loss:  tensor(0.7631, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 1  loss:  tensor(0.6677, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 2  loss:  tensor(0.7050, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 3  loss:  tensor(0.6287, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 4  loss:  tensor(0.6068, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 5  loss:  tensor(0.5991, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 6  loss:  tensor(0.5171, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 7  loss:  tensor(0.7072, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 8  loss:  tensor(0.6694, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 9  loss:  tensor(0.4735, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 10  loss:  tensor(0.5220, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 11  loss:  tensor(0.4854, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 12  loss:  tensor(0.6578, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 13  loss:  tensor(0.6176, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 14  loss:  tensor(0.5686, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 15  loss:  tensor(0.6186, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 16  loss:  tensor(0.6232, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 17  loss:  tensor(0.7542, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 18  loss:  tensor(0.5299, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 19  loss:  tensor(0.7498, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 20  loss:  tensor(0.7192, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 21  loss:  tensor(0.7699, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 22  loss:  tensor(0.5565, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 23  loss:  tensor(0.5458, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 24  loss:  tensor(0.7263, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 25  loss:  tensor(0.5359, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 26  loss:  tensor(0.6236, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 27  loss:  tensor(0.6473, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 28  loss:  tensor(0.5653, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 29  loss:  tensor(0.5721, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 30  loss:  tensor(0.5793, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 31  loss:  tensor(0.6897, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 32  loss:  tensor(0.5785, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 33  loss:  tensor(0.5849, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 34  loss:  tensor(0.6296, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 35  loss:  tensor(0.6873, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 36  loss:  tensor(0.5608, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 37  loss:  tensor(0.5434, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 38  loss:  tensor(0.6196, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 39  loss:  tensor(0.4699, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 40  loss:  tensor(0.4620, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 41  loss:  tensor(0.5523, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 42  loss:  tensor(0.6193, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 43  loss:  tensor(0.8277, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 44  loss:  tensor(0.6930, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 45  loss:  tensor(0.3774, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 46  loss:  tensor(0.6129, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 47  loss:  tensor(0.5165, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 48  loss:  tensor(0.4751, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 49  loss:  tensor(0.5146, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 50  loss:  tensor(0.6541, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 51  loss:  tensor(0.6129, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 52  loss:  tensor(0.4539, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 53  loss:  tensor(0.6459, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 54  loss:  tensor(0.5301, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 55  loss:  tensor(0.4283, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 56  loss:  tensor(0.4568, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 57  loss:  tensor(0.5442, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 58  loss:  tensor(0.4271, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 59  loss:  tensor(0.5050, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 60  loss:  tensor(0.4692, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 61  loss:  tensor(0.6213, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 62  loss:  tensor(0.3981, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 63  loss:  tensor(0.4205, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 64  loss:  tensor(0.6516, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 65  loss:  tensor(0.4472, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 66  loss:  tensor(0.4286, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 67  loss:  tensor(0.3502, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 68  loss:  tensor(0.6762, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 69  loss:  tensor(0.4540, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 70  loss:  tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 71  loss:  tensor(0.4095, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 72  loss:  tensor(0.4792, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 73  loss:  tensor(0.3227, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 74  loss:  tensor(0.2480, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 75  loss:  tensor(0.3265, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 76  loss:  tensor(0.4648, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 77  loss:  tensor(0.6497, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 78  loss:  tensor(0.5286, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 79  loss:  tensor(0.2390, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 80  loss:  tensor(0.3645, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 81  loss:  tensor(0.4220, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 82  loss:  tensor(0.3759, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 83  loss:  tensor(0.2989, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 84  loss:  tensor(0.2089, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 85  loss:  tensor(0.3284, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 86  loss:  tensor(0.3223, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 87  loss:  tensor(0.2970, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 88  loss:  tensor(0.2641, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 89  loss:  tensor(0.1921, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 90  loss:  tensor(0.2768, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 91  loss:  tensor(0.2542, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 92  loss:  tensor(0.0933, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 93  loss:  tensor(0.2529, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 94  loss:  tensor(0.3915, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 95  loss:  tensor(0.2963, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 96  loss:  tensor(0.5306, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 97  loss:  tensor(0.2383, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 98  loss:  tensor(0.2951, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 99  loss:  tensor(0.1404, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 100  loss:  tensor(0.3328, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 101  loss:  tensor(0.2022, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 102  loss:  tensor(0.1371, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 103  loss:  tensor(0.2043, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 104  loss:  tensor(0.1861, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 105  loss:  tensor(0.3282, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 106  loss:  tensor(0.3683, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 107  loss:  tensor(0.4326, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 108  loss:  tensor(0.2885, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 109  loss:  tensor(0.4157, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 110  loss:  tensor(0.1888, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 111  loss:  tensor(0.1880, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 112  loss:  tensor(0.1570, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 113  loss:  tensor(0.3376, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 114  loss:  tensor(0.1821, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 115  loss:  tensor(0.1991, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 116  loss:  tensor(0.1333, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 117  loss:  tensor(0.1123, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 118  loss:  tensor(0.3087, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Train Step: 119  loss:  tensor(0.3003, device='cuda:0', grad_fn=<BinaryCrossEntropyBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input_size = 1\n",
    "seq_len = 14\n",
    "batch_size = 20\n",
    "hidden_size = 28\n",
    "epochs = 120\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data = np.concatenate((sample, label),axis=1)\n",
    "data_train, data_test = train_test_split(data)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset = torch.utils.data.TensorDataset(torch.Tensor(data_train[:, :-1]).to(device), torch.Tensor(data_train[:, -1]).to(device)),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "model = VanillaRNN(input_size=input_size, hidden_size=hidden_size)\n",
    "loss_function = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(seq).squeeze()\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Train Step:\", i, \" loss: \", single_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8764940239043825\n",
      "0.8477389811104751\n",
      "0.881547619047619\n",
      "0.8643128100379341\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset = torch.utils.data.TensorDataset(torch.Tensor(data_test[:, :-1]).to(device), torch.Tensor(data_test[:, -1]).to(device)),\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False\n",
    ")\n",
    "\n",
    "pred = np.zeros(batch_size)\n",
    "true = np.zeros(batch_size)\n",
    "\n",
    "for seq, labels in test_loader:\n",
    "    y_pred = model(seq).squeeze()\n",
    "    pred = np.append(pred, np.round(y_pred.cpu().detach().numpy()))\n",
    "    true = np.append(true, labels.cpu().detach().numpy())\n",
    "\n",
    "print(accuracy_score(true, pred))\n",
    "print(precision_score(true, pred))\n",
    "print(recall_score(true, pred))\n",
    "print(f1_score(true, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11235/11235 [00:05<00:00, 2229.36it/s]\n",
      "100%|██████████| 3745/3745 [00:01<00:00, 2223.77it/s]\n"
     ]
    }
   ],
   "source": [
    "te = TakensEmbedding(time_delay=1, dimension=3)\n",
    "sample_train_tda = te.fit_transform(data_train[:, :-1])\n",
    "sample_test_tda = te.fit_transform(data_test[:, :-1])\n",
    "\n",
    "feature_train_list = []\n",
    "feature_test_list = []\n",
    "\n",
    "for i in tqdm(range(sample_train_tda.shape[0])):\n",
    "    r = ripser(sample_train_tda[i, :, :])\n",
    "    feature = np.delete(r['dgms'][0], -1, axis=0)[:, 1]\n",
    "    # feature2 = r['dgms'][1]\n",
    "    f = [feature.shape[0], np.sum(feature), np.mean(feature), np.std(feature), np.max(feature), np.min(feature), len([_ for _ in feature if _>0.5*np.max(feature)]), len([_ for _ in feature if _>np.mean(feature)])]\n",
    "    f = [np.round(_, 2) for _ in f]\n",
    "    feature_train_list.append(np.array(f))\n",
    "\n",
    "for i in tqdm(range(sample_test_tda.shape[0])):\n",
    "    r = ripser(sample_test_tda[i, :, :])\n",
    "    feature = np.delete(r['dgms'][0], -1, axis=0)[:, 1]\n",
    "    f = [feature.shape[0], np.sum(feature), np.mean(feature), np.std(feature), np.max(feature), np.min(feature), len([_ for _ in feature if _>0.5*np.max(feature)]), len([_ for _ in feature if _>np.mean(feature)])]\n",
    "    f = [np.round(_, 2) for _ in f]\n",
    "    feature_test_list.append(np.array(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.11420989 2.27933598]\n",
      " [2.09824705 2.13405943]]\n",
      "0.0\n",
      "[0.16512609 0.03581238]\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "r = ripser(sample_test_tda[i, :, :])\n",
    "print(r['dgms'][1])\n",
    "print(data_train[i, -1])\n",
    "\n",
    "feature2 = r['dgms'][1]\n",
    "sub = np.zeros(1)\n",
    "for i in range(feature2.shape[0]):\n",
    "    sub = np.append(sub, feature2[i, 1]-feature2[i, 0])\n",
    "\n",
    "\n",
    "\n",
    "# print(feature2.shape[0], np.min(feature2.flatten()), np.max(feature2.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_pd = pd.DataFrame(np.concatenate((np.array(feature_train_list), data_train[:, -1].reshape(-1, 1)), axis=1), \\\n",
    "    columns=['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average', 'Class'])\n",
    "\n",
    "print(feature_train_pd.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.3, kernel='linear')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = feature_train_pd[['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average']]\n",
    "Y_train = feature_train_pd[['Class']]\n",
    "\n",
    "clf = SVC(C=0.3, kernel='linear')\n",
    "clf.fit(X_train, np.ravel(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5703604806408544\n"
     ]
    }
   ],
   "source": [
    "feature_test_pd = pd.DataFrame(np.concatenate((np.array(feature_test_list), data_test[:, -1].reshape(-1, 1)), axis=1), \\\n",
    "    columns=['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average', 'Class'])\n",
    "X_test = feature_test_pd[['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average']]\n",
    "\n",
    "Y_test = feature_test_pd[['Class']]\n",
    "\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(accuracy_score(y_pred=Y_pred, y_true=Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ripser_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d856ea83737ecf31b084a6c4dd25de1f47360aa39d05be84e9035ad6a44a6c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
