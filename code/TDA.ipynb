{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import LinearSVC, SVC, SVR\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from gtda.time_series import TakensEmbedding\n",
    "from PyEMD import EMD\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from pylab import mpl\n",
    "from sklearn import preprocessing\n",
    "\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define load data func\n",
    "def load_data_from_csv(filepath: str) -> np.ndarray:\n",
    "    csv_file = pd.read_csv(filepath)\n",
    "    school_num = csv_file.shape[0]\n",
    "    year_num = csv_file.shape[1] - 2    # except first and last column\n",
    "    data_dim = len(csv_file.iloc[0, 1].strip().split())\n",
    "    school_names = tuple(csv_file[\"SchoolName\"])\n",
    "    data = np.zeros((school_num, year_num, data_dim))\n",
    "    for i, _ in enumerate(school_names):\n",
    "        data_row = list(csv_file.iloc[i, 1:year_num+1])\n",
    "        data_row = [_.strip().split() for _ in data_row]\n",
    "        data_row = [float(_) for l in data_row for _ in l]\n",
    "        data_row = np.array(data_row).reshape((year_num, -1))\n",
    "        data[i, :, :] = data_row\n",
    "    return data, school_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data, school_names = load_data_from_csv(\"../dataset/all-data.csv\")\n",
    "school_n, year_n, data_dim = data.shape\n",
    "idx2sch = {k:v for k,v in enumerate(school_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the data we use (see README.md)\n",
    "selected_data_idx = np.array([0, 2, 7, 11, 15, 19])\n",
    "selected_data = data[:, :, selected_data_idx]\n",
    "selected_data = selected_data.transpose(0, 2, 1)\n",
    "print(selected_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more data!\n",
    "more_data_path = \"../dataset/more-school-all.npy\"\n",
    "more_data = np.load(more_data_path)\n",
    "selected_data = np.concatenate((selected_data, more_data), axis=0)\n",
    "print(selected_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more data!\n",
    "more_data_path = \"../dataset/top20.npy\"\n",
    "more_data = np.load(more_data_path)\n",
    "selected_data = np.zeros((20,6,12))\n",
    "selected_data[:, 0:2, :] = more_data[:, 0:2, :]\n",
    "# print(selected_data[0,:,:])\n",
    "selected_data[:, 2, :] = more_data[:, 2, :] + more_data[:, 3, :]\n",
    "selected_data[:, 3:6, :] = more_data[:, 4:7, :]\n",
    "print(selected_data.shape)\n",
    "print(selected_data[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_data_scaled = preprocessing.scale(selected_data.reshape(-1, 10), axis=1)\n",
    "\n",
    "D_state_list = []\n",
    "G_state_list = []\n",
    "\n",
    "for i in range(selected_data_scaled.shape[0]):\n",
    "    if selected_data_scaled[i][-2] == 0:\n",
    "        G_state_list.append(i)\n",
    "    else:\n",
    "        if selected_data_scaled[i][-1]/selected_data_scaled[i][-2] > 1:\n",
    "            G_state_list.append(i)\n",
    "        else:\n",
    "            D_state_list.append(i)\n",
    "\n",
    "print(G_state_list)\n",
    "print(D_state_list)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "x_2d = pca.fit_transform(selected_data_scaled)\n",
    "g = plt.scatter(x_2d[G_state_list, 0], x_2d[G_state_list, 1], s=65, color='red', label=\"Samples with growing trend\")\n",
    "d = plt.scatter(x_2d[D_state_list, 0], x_2d[D_state_list, 1], s=65, color='royalblue', label=\"Samples with decreasing trend\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.axes.xaxis.set_ticklabels([])\n",
    "ax.axes.yaxis.set_ticklabels([])\n",
    "ax.spines[:].set_linewidth('2.0')\n",
    "\n",
    "plt.legend(fontsize=18)\n",
    "plt.title(\"Samples distribution in original space\", fontsize=24, pad=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trend-generation func\n",
    "def generate_trend_matrix(statis_matrix: np.array, dot_prsv=2) -> np.array:\n",
    "    assert len(statis_matrix.shape) == 2\n",
    "    m, n = statis_matrix.shape\n",
    "    trend_matrix = np.empty((m, n-1))\n",
    "    for row in range(0, m):\n",
    "        for col in range(0, n-1):\n",
    "            if statis_matrix[row, col] != 0:\n",
    "                trend_matrix[row, col] = round(statis_matrix[row, col+1] / statis_matrix[row, col], dot_prsv)\n",
    "            else:\n",
    "                trend_matrix[row, col] = 10\n",
    "    return trend_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enhanced_data = []\n",
    "reshaped = selected_data.reshape(-1, 10)\n",
    "# reshaped = [generate_trend_matrix(reshaped[i, :].reshape(1,-1)).squeeze() for i in range(reshaped.shape[0])]\n",
    "# reshaped = np.array(reshaped)\n",
    "for i in range(reshaped.shape[0]):\n",
    "    d = reshaped[i, :]\n",
    "    d = (d-np.mean(d)) / np.std(d)\n",
    "    # ATTENTION\n",
    "    enhanced_data.append([[d[j], d[j+1], d[j+2]] for j in range(len(d)-3)])\n",
    "    # enhanced_data.append([[d[j], d[j+1], d[j+2]] for j in range(len(d)-2)])\n",
    "enhanced_data = np.array(enhanced_data)\n",
    "print(enhanced_data.shape)\n",
    "print(enhanced_data[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['font.family'] = \"Microsoft Reference Sans Serif\"\n",
    "mpl.rcParams['legend.fontsize'] = 18\n",
    "i = 0\n",
    "r = ripser(enhanced_data[i,:,:])\n",
    "plt.subplot(2,3,1)\n",
    "plot_diagrams(r['dgms'], diagonal=False, size=40)\n",
    "\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['font.family'] = \"Microsoft Reference Sans Serif\"\n",
    "i = 3\n",
    "r = ripser(enhanced_data[i,:,:])\n",
    "plt.subplot(2,3,2)\n",
    "plot_diagrams(r['dgms'], diagonal=False, size=40)\n",
    "\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['font.family'] = \"Microsoft Reference Sans Serif\"\n",
    "i = 2\n",
    "r = ripser(enhanced_data[i,:,:])\n",
    "plt.subplot(2,3,3)\n",
    "plot_diagrams(r['dgms'], diagonal=False, size=40)\n",
    "\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['font.family'] = \"Microsoft Reference Sans Serif\"\n",
    "i = 4\n",
    "r = ripser(enhanced_data[i,:,:])\n",
    "plt.subplot(2,3,4)\n",
    "plot_diagrams(r['dgms'], diagonal=False, size=40)\n",
    "\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['font.family'] = \"Microsoft Reference Sans Serif\"\n",
    "i = 5\n",
    "r = ripser(enhanced_data[i,:,:])\n",
    "plt.subplot(2,3,5)\n",
    "plot_diagrams(r['dgms'], diagonal=False, size=40)\n",
    "\n",
    "mpl.rcParams['font.size'] = 18\n",
    "mpl.rcParams['font.family'] = \"Microsoft Reference Sans Serif\"\n",
    "i = 10\n",
    "r = ripser(enhanced_data[i,:,:])\n",
    "plt.subplot(2,3,6)\n",
    "plot_diagrams(r['dgms'], diagonal=False, size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(2):\n",
    "# for i in range(enhanced_data.shape[0]):\n",
    "feature_list = []\n",
    "for i in range(enhanced_data.shape[0]):\n",
    "    r = ripser(enhanced_data[i, :, :])\n",
    "    feature = np.delete(r['dgms'][0], -1, axis=0)[:, 1]\n",
    "    # feature_list.append(np.array([feature.shape[0], np.sum(feature), np.mean(feature), np.max(feature), np.min(feature), len([_ for _ in feature if _>0.5*np.max(feature)]), len([_ for _ in feature if _>np.mean(feature)])]))\n",
    "    f = [feature.shape[0], np.sum(feature), np.mean(feature), np.std(feature), np.max(feature), np.min(feature), len([_ for _ in feature if _>0.5*np.max(feature)]), len([_ for _ in feature if _>np.mean(feature)])]\n",
    "    f = [np.round(_, 2) for _ in f]\n",
    "    feature_list.append(np.array(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trends\n",
    "trends = np.array([generate_trend_matrix(reshaped[i, :].reshape(1,-1)).squeeze() for i in range(reshaped.shape[0])])\n",
    "trends = trends[:, -1].reshape(-1, 1)\n",
    "# print(trends)\n",
    "feature_pd = pd.DataFrame(np.concatenate((np.array(feature_list), trends), axis=1), \\\n",
    "    columns=['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average', 'Trend'])\n",
    "# print(feature_pd)\n",
    "# feature_pd.to_csv('./feature_map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum, Max and Min of Lifetime are selected\n",
    "print(feature_pd.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See classification bounds\n",
    "feature_np = np.array(feature_list)\n",
    "print(feature_np.shape)\n",
    "# data_1d = feature_np.reshape(1, -1).squeeze()\n",
    "# print(data_1d.shape)\n",
    "\n",
    "# data_1d = sorted(data_1d, reverse=False)\n",
    "\n",
    "# print(data_1d.index(1.0))\n",
    "# print(data_1d[0], data_1d[800], data_1d[1600])\n",
    "# print(data_1d[0], data_1d[600], data_1d[1200], data_1d[1800])\n",
    "\n",
    "D_state_list = []\n",
    "G_state_list = []\n",
    "\n",
    "for i in range(selected_data_scaled.shape[0]):\n",
    "    if selected_data_scaled[i][-2] == 0:\n",
    "        G_state_list.append(i)\n",
    "    else:\n",
    "        if selected_data_scaled[i][-1]/selected_data_scaled[i][-2] > 1:\n",
    "            G_state_list.append(i)\n",
    "        else:\n",
    "            D_state_list.append(i)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "feature_np_2d = pca.fit_transform(feature_np)\n",
    "g = plt.scatter(feature_np_2d[G_state_list, 0], feature_np_2d[G_state_list, 1], s=65, color='red', label=\"Samples with growing trend\")\n",
    "d = plt.scatter(feature_np_2d[D_state_list, 0], feature_np_2d[D_state_list, 1], s=65, color='royalblue', label=\"Samples with decreasing trend\")\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.axes.xaxis.set_ticklabels([])\n",
    "ax.axes.yaxis.set_ticklabels([])\n",
    "ax.spines[:].set_linewidth('2.0')\n",
    "\n",
    "plt.legend(fontsize=18)\n",
    "plt.title(\"Samples distribution in Persistence Diagrams\", fontsize=24, pad=20)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature_pd[['Sum of lifetime', 'Max of lifetime', 'Min of lifetime']]\n",
    "X = feature_pd[['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average']]\n",
    "Y = feature_pd[['Trend']]\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: trend <= 1\n",
    "# 1: 1 < trend <= 1.2\n",
    "# 2: 1.2 < trend <= 1.5\n",
    "# 3: 1.5 < trend\n",
    "# def P_idx(x):\n",
    "#     if x<=1:\n",
    "#         return 0\n",
    "#     elif x>1 and x<=1.25:\n",
    "#         return 1\n",
    "#     elif x>1.25 and x<=1.5:\n",
    "#         return 2\n",
    "#     else:\n",
    "#         return 3\n",
    "\n",
    "# def P_idx(x):\n",
    "#     if x<=1:\n",
    "#         return 0\n",
    "#     elif x>1 and x<=1.5:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 2\n",
    "\n",
    "def P_idx(x):\n",
    "    if x<=1:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from sklearn import svm\n",
    "\n",
    "# def gaussianKernelGramMatrixFull(X1, X2, sigma=0.1):\n",
    "#     \"\"\"(Pre)calculates Gram Matrix K\"\"\"\n",
    "\n",
    "#     gram_matrix = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "#     for i, x1 in enumerate(X1):\n",
    "#         for j, x2 in enumerate(X2):\n",
    "#             x1 = x1.flatten()\n",
    "#             x2 = x2.flatten()\n",
    "#             gram_matrix[i, j] = np.exp(- np.sum( np.power((x1 - x2),2) ) / float( 2*(sigma**2) ) )\n",
    "#     return gram_matrix\n",
    "\n",
    "# X=...\n",
    "# y=...\n",
    "# Xval=...\n",
    "\n",
    "# C=0.1\n",
    "# clf = svm.SVC(C = C, kernel=\"precomputed\")\n",
    "# model = clf.fit( gaussianKernelGramMatrixFull(X,X), y )\n",
    "\n",
    "# p = model.predict( gaussianKernelGramMatrixFull(Xval, X) )\n",
    "def gaussianKernelGramMatrixFull(X1, X2, sigma=0.1):\n",
    "    \"\"\"(Pre)calculates Gram Matrix K\"\"\"\n",
    "\n",
    "    gram_matrix = np.zeros((X1.shape[0], X2.shape[0]))\n",
    "    for i, x1 in enumerate(X1):\n",
    "        for j, x2 in enumerate(X2):\n",
    "            x1 = x1.flatten()\n",
    "            x2 = x2.flatten()\n",
    "            gram_matrix[i, j] = np.exp(- np.sum( np.power((x1 - x2),2) ) / float( 2*(sigma**2) ) )\n",
    "    return gram_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(list(Y_train['Trend']))\n",
    "# print(list(Y_test))\n",
    "Y_train = Y_train.applymap(P_idx)\n",
    "Y_test = Y_test.applymap(P_idx)\n",
    "\n",
    "C = 0.1\n",
    "clf = SVC(C=C, kernel='linear')\n",
    "clf.fit(X_train, np.ravel(Y_train))\n",
    "\n",
    "print(precision_score(y_true=Y_test, y_pred=clf.predict(X_test), average='macro'))\n",
    "print(recall_score(y_true=Y_test, y_pred=clf.predict(X_test), average='macro'))\n",
    "print(accuracy_score(y_true=Y_test, y_pred=clf.predict(X_test)))\n",
    "print(f1_score(y_true=Y_test, y_pred=clf.predict(X_test), average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=Y_test, y_pred=clf.predict(X_test))\n",
    "\n",
    "# 某类的FP：该列所有元素之和减去该列的TP.\n",
    "# 某类的FN：该行所有元素之和减去该行的TP.\n",
    "# 某类的TN：整个混淆矩阵之和减去该类的（TP+FP+FN）\n",
    "print(cm)\n",
    "precision = 7 / 12  # TP / TP+FP\n",
    "recall = 7 / 11     # TP / TP+FN\n",
    "accuracy = 42 / 75   # TP+TN / ALL\n",
    "f1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "print(precision, recall, accuracy, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Predict, do not use :)\n",
    "X = feature_pd[['Sum of lifetime', 'Max of lifetime', 'Min of lifetime']]\n",
    "Y = feature_pd[['Trend']]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state=1)\n",
    "linreg = LinearRegression()\n",
    "model = linreg.fit(X_train, Y_train)\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n",
    "Y_pred = linreg.predict(X_test)\n",
    "Y_test = np.array(Y_test)\n",
    "\n",
    "# accu = 0\n",
    "# for i in range(Y_pred.shape[0]):\n",
    "#     if P_idx(Y_pred[i]) == P_idx(Y_test[i]):\n",
    "#         accu += 1\n",
    "\n",
    "# print(accu / Y_pred.shape[0])\n",
    "\n",
    "# print(Y_pred)\n",
    "# print(Y_test)\n",
    "print(Y_pred - Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = feature_pd[['Sum of lifetime', 'Max of lifetime', 'Min of lifetime']]\n",
    "X = feature_pd[['Number of TDA barcode points', 'Sum of lifetime', 'Average of lifetime', 'Std of lifetime', 'Max of lifetime', \\\n",
    "        'Min of lifetime', 'Number of points bigger than 0.5*max', 'Number of points bigger than average']]\n",
    "Y = feature_pd[['Trend']]\n",
    "\n",
    "# Y = Y.applymap(P_idx)\n",
    "\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y)\n",
    "\n",
    "X = X.to_numpy()\n",
    "Y = Y.to_numpy()\n",
    "\n",
    "pred = []\n",
    "for i in range(120):\n",
    "    X_test = X[i,1:]\n",
    "    X_train = np.delete(X, -1, axis=1)\n",
    "    X_train = np.delete(X_train, i, axis=0)\n",
    "    Y_train = np.delete(Y, i, axis=0)\n",
    "    svr = SVR(C=1.0, epsilon=0.2, kernel='linear')\n",
    "    svr.fit(X_train, np.ravel(Y_train))\n",
    "    # print(X_test)\n",
    "    pred.append(float(svr.predict(X_test.reshape(1,-1))))\n",
    "\n",
    "# for i in range(120):\n",
    "#     X_test = X[i,1:]\n",
    "#     X_train = np.delete(X, -1, axis=1)\n",
    "#     X_train = np.delete(X_train, i, axis=0)\n",
    "#     Y_train = np.delete(Y, i, axis=0)\n",
    "    \n",
    "#     C = 0.1\n",
    "#     clf = SVC(C=C, kernel='linear')\n",
    "#     clf.fit(X_train, np.ravel(Y_train))\n",
    "#     pred.append(int(clf.predict(X_test.reshape(1,-1))))\n",
    "print(pred[:120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(8).reshape(2, 4)\n",
    "TE = TakensEmbedding(time_delay=1, dimension=3)\n",
    "print(X)\n",
    "print(TE.fit_transform(X))\n",
    "print(TE.fit_transform(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TE = TakensEmbedding(time_delay=1, dimension=3)\n",
    "enhanced_data = TE.fit_transform(reshaped[:, :-1])\n",
    "print(enhanced_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enhanced_data[0,:,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ripser_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d856ea83737ecf31b084a6c4dd25de1f47360aa39d05be84e9035ad6a44a6c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
